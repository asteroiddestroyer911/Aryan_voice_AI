{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPzQmbkcsU88mgZmFQPD0uX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asteroiddestroyer911/Aryan_voice_AI/blob/main/Oceanopedia(chat_only).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woc8FPlz7wFA",
        "outputId": "1bf2f7d5-ddfe-4275-b1b5-ec6e55ff4374"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸŒŠ MARINE BIODIVERSITY AI CHATBOT TRAINING\n",
            "============================================================\n",
            "ğŸ¤– Using Ollama Llama 3.2 3B Model\n",
            "ğŸ“š Training on AI in Ocean Conservation & Aquaculture Research\n",
            "============================================================\n",
            "âœ… All packages installed successfully!\n"
          ]
        }
      ],
      "source": [
        "# ğŸŒŠ Marine Biodiversity AI Chatbot Training\n",
        "# Complete setup for training with Llama 3.2 3B via Ollama\n",
        "\n",
        "print(\"ğŸŒŠ MARINE BIODIVERSITY AI CHATBOT TRAINING\")\n",
        "print(\"=\" * 60)\n",
        "print(\"ğŸ¤– Using Ollama Llama 3.2 3B Model\")\n",
        "print(\"ğŸ“š Training on AI in Ocean Conservation & Aquaculture Research\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Install all required packages\n",
        "!pip install -q langchain langchain-community langchain-ollama\n",
        "!pip install -q pypdf faiss-cpu sentence-transformers\n",
        "!pip install -q streamlit pyngrok gradio\n",
        "!pip install -q requests beautifulsoup4 numpy pandas\n",
        "\n",
        "print(\"âœ… All packages installed successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Ollama in Colab\n",
        "import subprocess\n",
        "import time\n",
        "import requests\n",
        "import json\n",
        "\n",
        "print(\"ğŸ¦™ Installing Ollama...\")\n",
        "\n",
        "# Download and install Ollama\n",
        "!curl -fsSL https://ollama.ai/install.sh | sh\n",
        "\n",
        "print(\"âœ… Ollama installed!\")\n",
        "\n",
        "# Start Ollama server in background\n",
        "import threading\n",
        "import os\n",
        "\n",
        "def start_ollama_server():\n",
        "    \"\"\"Start Ollama server in background\"\"\"\n",
        "    subprocess.run([\"ollama\", \"serve\"], capture_output=False)\n",
        "\n",
        "# Start server thread\n",
        "server_thread = threading.Thread(target=start_ollama_server, daemon=True)\n",
        "server_thread.start()\n",
        "\n",
        "print(\"ğŸ”„ Starting Ollama server...\")\n",
        "time.sleep(10)\n",
        "\n",
        "# Pull Llama 3.2 3B model\n",
        "print(\"ğŸ“¥ Downloading Llama 3.2 3B model (this may take a few minutes)...\")\n",
        "!ollama pull llama3.2:3b\n",
        "\n",
        "print(\"âœ… Llama 3.2 3B model ready!\")\n",
        "\n",
        "# Test Ollama connection\n",
        "try:\n",
        "    response = requests.post('http://localhost:11434/api/generate',\n",
        "                           json={'model': 'llama3.2:3b', 'prompt': 'Hello', 'stream': False})\n",
        "    if response.status_code == 200:\n",
        "        print(\"ğŸ‰ Ollama server running successfully!\")\n",
        "    else:\n",
        "        print(\"âš ï¸ Ollama server connection issue\")\n",
        "except Exception as e:\n",
        "    print(f\"âš ï¸ Ollama setup error: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-IDlYYC_hlN",
        "outputId": "1542dea5-b565-4ee8-afb1-fe26adb4dd14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ¦™ Installing Ollama...\n",
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "#########                                                                 13.3%curl: (56) OpenSSL SSL_read: Connection reset by peer, errno 104\n",
            "\n",
            "\n",
            "gzip: stdin: unexpected end of file\n",
            "tar: Unexpected EOF in archive\n",
            "tar: Unexpected EOF in archive\n",
            "tar: Error is not recoverable: exiting now\n",
            "âœ… Ollama installed!\n",
            "ğŸ”„ Starting Ollama server...\n",
            "ğŸ“¥ Downloading Llama 3.2 3B model (this may take a few minutes)...\n",
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n",
            "âœ… Llama 3.2 3B model ready!\n",
            "ğŸ‰ Ollama server running successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive and setup marine research data\n",
        "from google.colab import drive\n",
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"ğŸ”— Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "print(\"âœ… Google Drive mounted successfully!\")\n",
        "\n",
        "# Setup directories\n",
        "DRIVE_FOLDER = '/content/drive/MyDrive/Marine_Research_Data'\n",
        "COLAB_DATA_DIR = '/content/marine_data'\n",
        "VECTORSTORE_DIR = '/content/vectorstore'\n",
        "\n",
        "# Create directories\n",
        "os.makedirs(COLAB_DATA_DIR, exist_ok=True)\n",
        "os.makedirs(VECTORSTORE_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"ğŸ“ Created working directories:\")\n",
        "print(f\"   â€¢ Marine Data: {COLAB_DATA_DIR}\")\n",
        "print(f\"   â€¢ Vector Store: {VECTORSTORE_DIR}\")\n",
        "\n",
        "# Marine research PDF files\n",
        "pdf_files = [\n",
        "    'BIG-Data-SIH-1.pdf',  # AI in Ocean Conservation & Marine Research\n",
        "    'BIG-Data-SIH-2.pdf',  # AI-driven aquaculture review\n",
        "    'BIG-Data-SIH-3.pdf'   # GIS-based marine data integration\n",
        "]\n",
        "\n",
        "print(f\"\\nğŸ“‹ Looking for PDFs in: {DRIVE_FOLDER}\")\n",
        "print(\"ğŸ“„ Expected files:\")\n",
        "for pdf in pdf_files:\n",
        "    print(f\"   â€¢ {pdf}\")\n",
        "\n",
        "# Copy PDFs from Drive to Colab\n",
        "copied_files = []\n",
        "missing_files = []\n",
        "\n",
        "for pdf_file in pdf_files:\n",
        "    source_path = f\"{DRIVE_FOLDER}/{pdf_file}\"\n",
        "    dest_path = f\"{COLAB_DATA_DIR}/{pdf_file}\"\n",
        "\n",
        "    if os.path.exists(source_path):\n",
        "        shutil.copy2(source_path, dest_path)\n",
        "        copied_files.append(dest_path)\n",
        "        print(f\"âœ… Copied: {pdf_file}\")\n",
        "    else:\n",
        "        missing_files.append(pdf_file)\n",
        "        print(f\"âŒ Missing: {pdf_file}\")\n",
        "\n",
        "print(f\"\\nğŸ“Š Summary:\")\n",
        "print(f\"   â€¢ Copied: {len(copied_files)} files\")\n",
        "print(f\"   â€¢ Missing: {len(missing_files)} files\")\n",
        "\n",
        "if missing_files:\n",
        "    print(f\"\\nğŸ“ To fix missing files:\")\n",
        "    print(f\"   1. Upload the missing PDFs to: {DRIVE_FOLDER}\")\n",
        "    print(f\"   2. Re-run this cell\")\n",
        "    print(f\"   Missing files: {', '.join(missing_files)}\")\n",
        "else:\n",
        "    print(f\"\\nğŸ‰ All marine research PDFs ready for training!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nXWfcIP_4yB",
        "outputId": "cdf6673e-bd2a-4fc0-a9c6-21ac5b571369"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”— Mounting Google Drive...\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "âœ… Google Drive mounted successfully!\n",
            "ğŸ“ Created working directories:\n",
            "   â€¢ Marine Data: /content/marine_data\n",
            "   â€¢ Vector Store: /content/vectorstore\n",
            "\n",
            "ğŸ“‹ Looking for PDFs in: /content/drive/MyDrive/Marine_Research_Data\n",
            "ğŸ“„ Expected files:\n",
            "   â€¢ BIG-Data-SIH-1.pdf\n",
            "   â€¢ BIG-Data-SIH-2.pdf\n",
            "   â€¢ BIG-Data-SIH-3.pdf\n",
            "âŒ Missing: BIG-Data-SIH-1.pdf\n",
            "âŒ Missing: BIG-Data-SIH-2.pdf\n",
            "âŒ Missing: BIG-Data-SIH-3.pdf\n",
            "\n",
            "ğŸ“Š Summary:\n",
            "   â€¢ Copied: 0 files\n",
            "   â€¢ Missing: 3 files\n",
            "\n",
            "ğŸ“ To fix missing files:\n",
            "   1. Upload the missing PDFs to: /content/drive/MyDrive/Marine_Research_Data\n",
            "   2. Re-run this cell\n",
            "   Missing files: BIG-Data-SIH-1.pdf, BIG-Data-SIH-2.pdf, BIG-Data-SIH-3.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Marine Biodiversity Chatbot with Ollama Llama 3.2 3B\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_ollama import OllamaLLM\n",
        "from langchain.prompts import PromptTemplate\n",
        "import pickle\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "class MarineBiodiversityChatbot:\n",
        "    def __init__(self, pdf_directory=COLAB_DATA_DIR, vectorstore_path=VECTORSTORE_DIR):\n",
        "        self.pdf_directory = pdf_directory\n",
        "        self.vectorstore_path = vectorstore_path\n",
        "        self.documents = []\n",
        "        self.vectorstore = None\n",
        "        self.qa_chain = None\n",
        "        self.llm = None\n",
        "\n",
        "        # Marine domain knowledge\n",
        "        self.marine_expertise = {\n",
        "            \"aquaculture\": [\"fish farming\", \"cultivation\", \"feeding\", \"growth\", \"health monitoring\", \"precision aquaculture\"],\n",
        "            \"conservation\": [\"biodiversity\", \"ecosystem\", \"species protection\", \"habitat\", \"marine protected areas\"],\n",
        "            \"ai_applications\": [\"computer vision\", \"machine learning\", \"predictive analytics\", \"automation\", \"sensor networks\"],\n",
        "            \"ocean_monitoring\": [\"water quality\", \"environmental parameters\", \"data integration\", \"remote sensing\", \"GIS\"],\n",
        "            \"sustainability\": [\"sustainable fishing\", \"ecosystem management\", \"climate change\", \"pollution control\"]\n",
        "        }\n",
        "\n",
        "        print(\"ğŸŒŠ Marine Biodiversity Chatbot initialized!\")\n",
        "        print(f\"ğŸ“ PDF Directory: {self.pdf_directory}\")\n",
        "        print(f\"ğŸ’¾ Vector Store: {self.vectorstore_path}\")\n",
        "\n",
        "    def setup_llm(self):\n",
        "        \"\"\"Initialize Ollama Llama 3.2 3B model\"\"\"\n",
        "        print(\"ğŸ¦™ Setting up Llama 3.2 3B model...\")\n",
        "\n",
        "        try:\n",
        "            # Initialize Ollama LLM\n",
        "            self.llm = OllamaLLM(\n",
        "                model=\"llama3.2:3b\",\n",
        "                base_url=\"http://localhost:11434\",\n",
        "                temperature=0.3,\n",
        "                top_p=0.9,\n",
        "                repeat_penalty=1.1\n",
        "            )\n",
        "\n",
        "            # Test the model\n",
        "            test_response = self.llm.invoke(\"Hello, I'm a marine science AI assistant.\")\n",
        "            print(\"âœ… Llama 3.2 3B model ready!\")\n",
        "            print(f\"ğŸ§ª Test response: {test_response[:100]}...\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error setting up Llama model: {e}\")\n",
        "            return False\n",
        "\n",
        "    def load_marine_documents(self):\n",
        "        \"\"\"Load and process marine research PDFs\"\"\"\n",
        "        print(\"\\nğŸ“š Loading marine research documents...\")\n",
        "\n",
        "        pdf_files = [f for f in os.listdir(self.pdf_directory) if f.endswith('.pdf')]\n",
        "\n",
        "        if not pdf_files:\n",
        "            print(f\"âŒ No PDF files found in {self.pdf_directory}\")\n",
        "            return False\n",
        "\n",
        "        total_pages = 0\n",
        "        for pdf_file in pdf_files:\n",
        "            pdf_path = os.path.join(self.pdf_directory, pdf_file)\n",
        "            print(f\"ğŸ“„ Processing: {pdf_file}\")\n",
        "\n",
        "            try:\n",
        "                loader = PyPDFLoader(pdf_path)\n",
        "                docs = loader.load()\n",
        "\n",
        "                # Add source metadata\n",
        "                for doc in docs:\n",
        "                    doc.metadata['source_file'] = pdf_file\n",
        "                    doc.metadata['domain'] = 'marine_science'\n",
        "\n",
        "                self.documents.extend(docs)\n",
        "                total_pages += len(docs)\n",
        "                print(f\"   âœ… Loaded {len(docs)} pages\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"   âŒ Error loading {pdf_file}: {str(e)}\")\n",
        "\n",
        "        print(f\"\\nğŸ“Š Document Loading Summary:\")\n",
        "        print(f\"   â€¢ Total PDFs processed: {len(pdf_files)}\")\n",
        "        print(f\"   â€¢ Total pages loaded: {total_pages}\")\n",
        "        print(f\"   â€¢ Documents in memory: {len(self.documents)}\")\n",
        "\n",
        "        return len(self.documents) > 0\n",
        "\n",
        "    def create_vector_database(self):\n",
        "        \"\"\"Create FAISS vector database from marine documents\"\"\"\n",
        "        print(\"\\nğŸ§  Creating marine knowledge vector database...\")\n",
        "\n",
        "        if not self.documents:\n",
        "            print(\"âŒ No documents loaded. Run load_marine_documents() first.\")\n",
        "            return False\n",
        "\n",
        "        # Advanced text splitting for better context preservation\n",
        "        text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=1500,  # Larger chunks for better context\n",
        "            chunk_overlap=300,  # More overlap for continuity\n",
        "            separators=[\"\\n\\n\", \"\\n\", \". \", \"! \", \"? \", \" \", \"\"],\n",
        "            keep_separator=True\n",
        "        )\n",
        "\n",
        "        print(\"âœ‚ï¸ Splitting documents into chunks...\")\n",
        "        text_chunks = text_splitter.split_documents(self.documents)\n",
        "\n",
        "        print(f\"ğŸ“ Created {len(text_chunks)} knowledge chunks\")\n",
        "\n",
        "        # Create embeddings with marine-optimized model\n",
        "        print(\"ğŸ”¤ Creating embeddings...\")\n",
        "        embeddings = HuggingFaceEmbeddings(\n",
        "            model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "            model_kwargs={'device': 'cpu'}\n",
        "        )\n",
        "\n",
        "        # Create vector store\n",
        "        print(\"ğŸ—„ï¸ Building FAISS vector database...\")\n",
        "        self.vectorstore = FAISS.from_documents(text_chunks, embeddings)\n",
        "\n",
        "        # Save vector store\n",
        "        vectorstore_file = os.path.join(self.vectorstore_path, \"marine_vectorstore\")\n",
        "        self.vectorstore.save_local(vectorstore_file)\n",
        "\n",
        "        print(f\"ğŸ’¾ Vector database saved to: {vectorstore_file}\")\n",
        "        print(\"âœ… Marine knowledge database created successfully!\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    def setup_qa_chain(self):\n",
        "        \"\"\"Setup QA chain with marine-specific prompting\"\"\"\n",
        "        print(\"\\nğŸ”— Setting up marine expertise QA chain...\")\n",
        "\n",
        "        if not self.llm:\n",
        "            print(\"âŒ LLM not initialized. Run setup_llm() first.\")\n",
        "            return False\n",
        "\n",
        "        if not self.vectorstore:\n",
        "            print(\"âŒ Vector store not created. Run create_vector_database() first.\")\n",
        "            return False\n",
        "\n",
        "        # Marine-specific prompt template\n",
        "        marine_prompt = PromptTemplate(\n",
        "            input_variables=[\"context\", \"question\"],\n",
        "            template=\"\"\"You are a Marine Biodiversity and Aquaculture AI Expert with deep knowledge of:\n",
        "â€¢ AI applications in ocean conservation and marine research\n",
        "â€¢ Advanced aquaculture technologies and precision farming\n",
        "â€¢ Marine ecosystem monitoring and data integration\n",
        "â€¢ Sustainable fisheries management and conservation strategies\n",
        "\n",
        "Use the following research context to answer the question accurately and comprehensively:\n",
        "\n",
        "RESEARCH CONTEXT:\n",
        "{context}\n",
        "\n",
        "QUESTION: {question}\n",
        "\n",
        "EXPERT RESPONSE:\n",
        "Provide a detailed, scientifically accurate answer based on the research context. Include:\n",
        "1. Direct answers with specific details from the research\n",
        "2. Practical applications and real-world examples\n",
        "3. Technical explanations when relevant\n",
        "4. Future implications and opportunities\n",
        "\n",
        "If the question is outside the research scope, clearly state this and provide general marine science guidance.\n",
        "\n",
        "Answer:\"\"\"\n",
        "        )\n",
        "\n",
        "        # Create QA chain\n",
        "        self.qa_chain = RetrievalQA.from_chain_type(\n",
        "            llm=self.llm,\n",
        "            chain_type=\"stuff\",\n",
        "            retriever=self.vectorstore.as_retriever(\n",
        "                search_type=\"similarity\",\n",
        "                search_kwargs={\"k\": 4}  # Retrieve top 4 most relevant chunks\n",
        "            ),\n",
        "            return_source_documents=True,\n",
        "            chain_type_kwargs={\"prompt\": marine_prompt}\n",
        "        )\n",
        "\n",
        "        print(\"âœ… Marine expertise QA chain ready!\")\n",
        "        return True\n",
        "\n",
        "    def ask_marine_question(self, question: str):\n",
        "        \"\"\"Ask a question to the marine AI expert\"\"\"\n",
        "        if not self.qa_chain:\n",
        "            return {\n",
        "                \"answer\": \"âŒ QA chain not initialized. Please run the full setup first.\",\n",
        "                \"sources\": [],\n",
        "                \"confidence\": \"low\"\n",
        "            }\n",
        "\n",
        "        try:\n",
        "            print(f\"ğŸ” Analyzing question: {question}\")\n",
        "\n",
        "            # Get response from QA chain\n",
        "            result = self.qa_chain({\"query\": question})\n",
        "\n",
        "            # Extract source information\n",
        "            sources = []\n",
        "            for doc in result[\"source_documents\"]:\n",
        "                source_info = {\n",
        "                    \"file\": doc.metadata.get(\"source_file\", \"Unknown\"),\n",
        "                    \"page\": doc.metadata.get(\"page\", \"Unknown\"),\n",
        "                    \"content_preview\": doc.page_content[:200] + \"...\"\n",
        "                }\n",
        "                sources.append(source_info)\n",
        "\n",
        "            return {\n",
        "                \"answer\": result[\"result\"],\n",
        "                \"sources\": sources,\n",
        "                \"confidence\": \"high\",\n",
        "                \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                \"answer\": f\"âš ï¸ Error processing question: {str(e)}\",\n",
        "                \"sources\": [],\n",
        "                \"confidence\": \"low\"\n",
        "            }\n",
        "\n",
        "    def get_training_summary(self):\n",
        "        \"\"\"Get summary of training data and capabilities\"\"\"\n",
        "        return {\n",
        "            \"documents_loaded\": len(self.documents),\n",
        "            \"vector_database\": \"FAISS with sentence-transformers embeddings\",\n",
        "            \"llm_model\": \"Ollama Llama 3.2 3B\",\n",
        "            \"expertise_areas\": list(self.marine_expertise.keys()),\n",
        "            \"status\": \"Ready for marine biodiversity and aquaculture questions\"\n",
        "        }\n",
        "\n",
        "print(\"ğŸŒŠ Marine Biodiversity Chatbot class defined!\")\n",
        "print(\"ğŸš€ Ready for training and deployment!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hf3C-FxFAF8D",
        "outputId": "e1f9ba95-f2d8-4de7-d5dd-b54cbe8f85aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸŒŠ Marine Biodiversity Chatbot class defined!\n",
            "ğŸš€ Ready for training and deployment!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete training pipeline for Marine Biodiversity Chatbot\n",
        "print(\"ğŸŒŠ STARTING MARINE CHATBOT TRAINING PIPELINE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Initialize chatbot\n",
        "marine_bot = MarineBiodiversityChatbot()\n",
        "\n",
        "# Step 1: Setup Llama 3.2 3B model\n",
        "print(\"\\nğŸ¦™ STEP 1: Setting up Llama 3.2 3B model...\")\n",
        "if marine_bot.setup_llm():\n",
        "    print(\"âœ… Llama 3.2 3B ready for marine expertise!\")\n",
        "else:\n",
        "    print(\"âŒ Failed to setup Llama model. Check Ollama installation.\")\n",
        "    raise Exception(\"LLM setup failed\")\n",
        "\n",
        "# Step 2: Load marine research documents\n",
        "print(\"\\nğŸ“š STEP 2: Loading marine research documents...\")\n",
        "if marine_bot.load_marine_documents():\n",
        "    print(\"âœ… Marine research data loaded successfully!\")\n",
        "else:\n",
        "    print(\"âŒ Failed to load documents. Check PDF files.\")\n",
        "    raise Exception(\"Document loading failed\")\n",
        "\n",
        "# Step 3: Create vector database\n",
        "print(\"\\nğŸ§  STEP 3: Creating marine knowledge vector database...\")\n",
        "if marine_bot.create_vector_database():\n",
        "    print(\"âœ… Marine knowledge database created!\")\n",
        "else:\n",
        "    print(\"âŒ Failed to create vector database.\")\n",
        "    raise Exception(\"Vector database creation failed\")\n",
        "\n",
        "# Step 4: Setup QA chain\n",
        "print(\"\\nğŸ”— STEP 4: Setting up marine expertise QA chain...\")\n",
        "if marine_bot.setup_qa_chain():\n",
        "    print(\"âœ… Marine AI expert QA chain ready!\")\n",
        "else:\n",
        "    print(\"âŒ Failed to setup QA chain.\")\n",
        "    raise Exception(\"QA chain setup failed\")\n",
        "\n",
        "# Training completion\n",
        "print(\"\\nğŸ‰ TRAINING COMPLETED SUCCESSFULLY!\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Display training summary\n",
        "summary = marine_bot.get_training_summary()\n",
        "print(\"\\nğŸ“Š TRAINING SUMMARY:\")\n",
        "print(f\"   â€¢ Documents processed: {summary['documents_loaded']}\")\n",
        "print(f\"   â€¢ Vector database: {summary['vector_database']}\")\n",
        "print(f\"   â€¢ LLM model: {summary['llm_model']}\")\n",
        "print(f\"   â€¢ Expertise areas: {', '.join(summary['expertise_areas'])}\")\n",
        "print(f\"   â€¢ Status: {summary['status']}\")\n",
        "\n",
        "print(\"\\nğŸŒŠ Your Marine Biodiversity AI Expert is ready to answer questions!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "9f-E-oQQALTx",
        "outputId": "85577c7d-67d7-4570-b4a3-45e90d69e192"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸŒŠ STARTING MARINE CHATBOT TRAINING PIPELINE\n",
            "============================================================\n",
            "ğŸŒŠ Marine Biodiversity Chatbot initialized!\n",
            "ğŸ“ PDF Directory: /content/marine_data\n",
            "ğŸ’¾ Vector Store: /content/vectorstore\n",
            "\n",
            "ğŸ¦™ STEP 1: Setting up Llama 3.2 3B model...\n",
            "ğŸ¦™ Setting up Llama 3.2 3B model...\n",
            "âœ… Llama 3.2 3B model ready!\n",
            "ğŸ§ª Test response: That's fascinating! As a marine science AI assistant, you must have access to a vast amount of knowl...\n",
            "âœ… Llama 3.2 3B ready for marine expertise!\n",
            "\n",
            "ğŸ“š STEP 2: Loading marine research documents...\n",
            "\n",
            "ğŸ“š Loading marine research documents...\n",
            "âŒ No PDF files found in /content/marine_data\n",
            "âŒ Failed to load documents. Check PDF files.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "Document loading failed",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1465636992.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"âŒ Failed to load documents. Check PDF files.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Document loading failed\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Step 3: Create vector database\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: Document loading failed"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the trained Marine Biodiversity Chatbot\n",
        "print(\"ğŸ§ª TESTING MARINE BIODIVERSITY AI EXPERT\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Comprehensive test questions covering different marine domains\n",
        "test_questions = [\n",
        "    # Aquaculture AI Applications\n",
        "    \"How can AI improve fish health monitoring in aquaculture systems?\",\n",
        "\n",
        "    # Marine Conservation Technology\n",
        "    \"What are the key AI technologies used for marine biodiversity conservation?\",\n",
        "\n",
        "    # Ocean Data Integration\n",
        "    \"How does GIS-based integration help in marine ecosystem management?\",\n",
        "\n",
        "    # Sustainable Fisheries\n",
        "    \"What role does AI play in preventing overfishing and promoting sustainable fisheries?\",\n",
        "\n",
        "    # Species Identification\n",
        "    \"How do computer vision techniques help in automated marine species identification?\"\n",
        "]\n",
        "\n",
        "print(f\"ğŸ“‹ Running {len(test_questions)} comprehensive tests...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Run test questions\n",
        "for i, question in enumerate(test_questions, 1):\n",
        "    print(f\"\\nğŸ”¬ TEST {i}: {question}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    # Get response from marine chatbot\n",
        "    response = marine_bot.ask_marine_question(question)\n",
        "\n",
        "    # Display response\n",
        "    print(\"ğŸ¤– MARINE AI EXPERT RESPONSE:\")\n",
        "    print(response[\"answer\"])\n",
        "\n",
        "    # Show source information\n",
        "    if response[\"sources\"]:\n",
        "        print(f\"\\nğŸ“š Sources ({len(response['sources'])} references):\")\n",
        "        for j, source in enumerate(response[\"sources\"], 1):\n",
        "            print(f\"   {j}. {source['file']} (Page: {source['page']})\")\n",
        "\n",
        "    print(f\"\\nâœ… Confidence: {response['confidence']}\")\n",
        "    print(f\"â° Generated at: {response['timestamp']}\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "print(\"\\nğŸ‰ ALL TESTS COMPLETED!\")\n",
        "print(\"âœ… Your Marine Biodiversity Chatbot is fully functional and ready for use!\")\n"
      ],
      "metadata": {
        "id": "kUDF-WxlBsy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Interactive chat interface for Marine Biodiversity AI Expert\n",
        "def start_marine_chat_interface():\n",
        "    \"\"\"Interactive chat with the marine biodiversity expert\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"ğŸŒŠ\" * 30)\n",
        "    print(\"WELCOME TO MARINE BIODIVERSITY AI EXPERT\")\n",
        "    print(\"ğŸŒŠ\" * 30)\n",
        "    print(\"\\nğŸ¤– Powered by Llama 3.2 3B + Marine Research Knowledge\")\n",
        "    print(\"ğŸ“š Trained on cutting-edge AI applications in marine science\")\n",
        "\n",
        "    print(\"\\nğŸ¯ I can help you with:\")\n",
        "    print(\"   ğŸŸ AI applications in aquaculture and fish farming\")\n",
        "    print(\"   ğŸŒŠ Marine conservation and ecosystem protection\")\n",
        "    print(\"   ğŸ“Š Ocean data integration and GIS analysis\")\n",
        "    print(\"   ğŸ£ Sustainable fisheries management\")\n",
        "    print(\"   ğŸ”¬ Marine species identification technologies\")\n",
        "    print(\"   ğŸ¤– AI and machine learning in ocean research\")\n",
        "\n",
        "    print(\"\\nğŸ’¡ Example questions:\")\n",
        "    print(\"   â€¢ How can AI optimize fish feeding in aquaculture?\")\n",
        "    print(\"   â€¢ What computer vision methods are used for coral monitoring?\")\n",
        "    print(\"   â€¢ How does predictive analytics help marine conservation?\")\n",
        "\n",
        "    print(\"\\nğŸ“ Type 'quit' to exit the chat\")\n",
        "    print(\"ğŸŒŠ\" + \"-\" * 58 + \"ğŸŒŠ\\n\")\n",
        "\n",
        "    chat_count = 0\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            # Get user input\n",
        "            user_question = input(\"ğŸŒŠ Your marine research question: \").strip()\n",
        "\n",
        "            # Check for exit commands\n",
        "            if user_question.lower() in ['quit', 'exit', 'bye', 'stop']:\n",
        "                print(\"\\nğŸ™ Thank you for exploring marine AI with us!\")\n",
        "                print(\"ğŸŒŠ Keep protecting our oceans! ğŸŒŠ\")\n",
        "                break\n",
        "\n",
        "            # Skip empty questions\n",
        "            if not user_question:\n",
        "                print(\"â“ Please ask a question about marine science!\")\n",
        "                continue\n",
        "\n",
        "            chat_count += 1\n",
        "            print(f\"\\nğŸ” [Chat {chat_count}] Consulting marine research database...\")\n",
        "\n",
        "            # Get response from chatbot\n",
        "            response = marine_bot.ask_marine_question(user_question)\n",
        "\n",
        "            # Display formatted response\n",
        "            print(\"\\n\" + \"ğŸ¤– MARINE AI EXPERT:\")\n",
        "            print(\"-\" * 50)\n",
        "            print(response[\"answer\"])\n",
        "\n",
        "            # Show sources if available\n",
        "            if response[\"sources\"]:\n",
        "                print(f\"\\nğŸ“š Research Sources:\")\n",
        "                for i, source in enumerate(response[\"sources\"], 1):\n",
        "                    print(f\"   {i}. {source['file']}\")\n",
        "\n",
        "            print(f\"\\nğŸ¯ Confidence: {response['confidence']}\")\n",
        "            print(\"=\" * 80)\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\n\\nğŸ™ Chat interrupted. Thanks for using Marine AI Expert!\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"\\nâš ï¸ Error: {str(e)}\")\n",
        "            print(\"Please try asking your question again.\")\n",
        "\n",
        "# Start interactive chat\n",
        "start_marine_chat_interface()\n"
      ],
      "metadata": {
        "id": "HyvoBpPwB-yA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}